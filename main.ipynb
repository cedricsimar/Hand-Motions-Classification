{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand motions classification using non-invasive EEG recordings\n",
    "### by Cedric Simar and Antoine Passemiers\n",
    "<hr/>\n",
    "\n",
    "## Table of content\n",
    "\n",
    "* [0 - Introduction](#introduction)\n",
    "  * [0.1. Problem description](#problem-description)\n",
    "\n",
    "\n",
    "* [1 - Preprocessing](#preprocessing)\n",
    "  * [1.1. Import useful libraries](#import-libraries)\n",
    "  * [1.2. Load the data](#load-data)\n",
    "  * [1.3. Low-pass filtering](#low-pass-filtering)\n",
    "  * [1.4. Downsampling](#downsampling)\n",
    "\n",
    "\n",
    "* [2 - A first naive approach: learn from raw EEG samples](#learn-from-raw-data)\n",
    "  * [2.1. Random Forests (first model)](#random-forests)\n",
    "    * [2.1.1 VC-dimension of random forests](#random-forests-vc)\n",
    "    * [2.1.2 Random forest pruning](#random-forest-pruning)\n",
    "  * [2.2  Logistic regression (second model)](#logistic-regression)\n",
    "    * [2.2.1 VC dimension of logistic regression](#logistic-regression-vc)\n",
    "\n",
    "\n",
    "* [3. Riemannian-based kernel trick](#kernel-trick)\n",
    "  * [3.1. Riemannian-based SVM (fourth model)](#ghmm)\n",
    "  * [3.2. Linear Discriminant Analysis (fifth model)](#lda)\n",
    "\n",
    "\n",
    "* [4 - Dealing with non-stationary data](#non-stationary-data)\n",
    "  * [4.1. Gaussian Hidden Markov Models (third model)](#ghmm)\n",
    "\n",
    "\n",
    "* [5. ANN, LSTM and weird stuff](#CAFE)\n",
    "\n",
    "\n",
    "* [X. Bibliography](#bibliography)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "<hr/>\n",
    "\n",
    "Note: This notebook has been designed in order to serve several purposes:\n",
    "1. Introduce different models either that have been successfully applied to EEG-based problems in the past or that are considered by us as being relevant given the nature of the task.\n",
    "2. Constantly refer to generalization theory while selecting our best models using Structural Risk Minimization (SRM)\n",
    "3. Present code excerpts used to achieve good performance on the [Kaggle competition](#bib-kaggle) about Grasp-and-Lift EEG detection.\n",
    "\n",
    "### Problem description <a class=\"anchor\" id=\"problem-description\"></a>\n",
    "\n",
    "Some patients have lost the use of one of their hand, and this may be due either by neurological handicaps or an amputation.\n",
    "The research goal of the present competition is to give those patients their full autonomy back by developing BCI (Brain-Computer Interface) devices capable of playing the role of a hand. This requires to design algorithms capable of mapping brain waves to the desired movement. In the framework of this competition, there are 6 different movements that one must be able to identify: 'HandStart', 'FirstDigitTouch', 'BothStartLoadPhase', 'LiftOff', 'Replace', and 'BothReleased'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe style=\"display:block\" width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/y3_Izuop2gY?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "url = \"https://www.youtube.com/embed/y3_Izuop2gY?rel=0&amp;controls=0&amp;showinfo=0\"\n",
    "HTML('<iframe style=\"display:block\" width=\"560\" height=\"315\" src=\"%s\" frameborder=\"0\" allowfullscreen></iframe>' % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "<hr/>\n",
    "\n",
    "### Import useful libraries <a class=\"anchor\" id=\"import-libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "\n",
    "import cython\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import scipy.linalg\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data <a class=\"anchor\" id=\"load-data\"></a>\n",
    "\n",
    "TODO Cedric: descriptions (electrodes, multi-labels, échantillonage, tout ça tout ça)\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"imgs/EEG_Electrode_Numbering.jpg\" style=\"width:450px;\">\n",
    "  <figcaption> Source: [Kaggle](#bib-kaggle) </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_PATIENTS = 12\n",
    "# Feature names\n",
    "ELECTRODE_NAMES = [\n",
    "    'Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "    'T7', 'C3', 'Cz', 'C4', 'T8', 'TP9', 'CP5', 'CP1', 'CP2', 'CP6', 'TP10',\n",
    "    'P7', 'P3', 'Pz', 'P4', 'P8', 'PO9', 'O1', 'Oz', 'O2', 'PO10']\n",
    "# Label names\n",
    "EVENT_NAMES = ['HandStart', 'FirstDigitTouch', 'BothStartLoadPhase', 'LiftOff', 'Replace', 'BothReleased']\n",
    "\n",
    "\n",
    "def load_dataset(subject=1, data_dir='', series=range(1,9)):\n",
    "    data, events = list(), list()\n",
    "    for series_id, s in enumerate(series):\n",
    "        print(\"Load series {0} from patient {1}\".format(series_id+1, subject))\n",
    "        data_filepath = os.path.join(data_dir, \"train/subj{0}_series{1}_data.csv\".format(subject, s))\n",
    "        data.append(pd.read_csv(data_filepath, index_col=0).values.astype(np.float))\n",
    "        events_filepath = os.path.join(data_dir, \"train/subj{0}_series{1}_events.csv\".format(subject, s))\n",
    "        events.append(pd.read_csv(events_filepath, index_col=0).values)\n",
    "    return data, np.concatenate(events, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load series 1 from patient 1\n",
      "Load series 2 from patient 1\n",
      "Load series 3 from patient 1\n",
      "Load series 4 from patient 1\n",
      "Load series 5 from patient 1\n",
      "Load series 6 from patient 1\n",
      "Load series 7 from patient 1\n",
      "Load series 8 from patient 1\n"
     ]
    }
   ],
   "source": [
    "data, events = load_dataset(subject=1, data_dir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-pass filtering <a class=\"anchor\" id=\"low-pass-filtering\"></a>\n",
    "\n",
    "To avoid creating potential aliasing effects, we apply necessary spectral filters on the raw signals before to downsample them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LowPassFilter:\n",
    "    \n",
    "    def __init__(self, cutoff_freq, sampling_freq, order=3):\n",
    "        self.nyquist_freq = sampling_freq / 2.\n",
    "        bound = cutoff_freq / self.nyquist_freq\n",
    "        self.b, self.a = scipy.signal.butter(order, bound, btype='lowpass', analog=True)\n",
    "    \n",
    "    def filter(self, signal):\n",
    "        # TODO: does lfiter include future data when filtering current frame?\n",
    "        return scipy.signal.lfilter(self.b, self.a, signal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fp1 = [series[:, 0] for series in data] # TODO: electrode 0 or 15\n",
    "filtered = list()\n",
    "for cutoff_freq in np.linspace(0, 1, 11)[1:]:\n",
    "    lowpass = LowPassFilter(cutoff_freq, 500)\n",
    "    filtered.append(np.concatenate([lowpass.filter(series_fp1) for series_fp1 in Fp1], axis=0)[:, np.newaxis])\n",
    "filtered = np.concatenate(filtered, axis=1)\n",
    "X_train_raw = np.concatenate((np.concatenate(data, axis=0), filtered, filtered ** 2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling <a class=\"anchor\" id=\"downsampling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_downsampled = X_train_raw[::4000]\n",
    "y_downsampled = events[::4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_downsampled = StandardScaler(copy=False).fit_transform(X_train_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val = X_train_downsampled[:200], X_train_downsampled[200:]\n",
    "y_train, y_val = y_downsampled[:200], y_downsampled[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first naive approach: learn from EEG samples <a class=\"anchor\" id=\"learn-from-raw-data\"></a>\n",
    "<hr/>\n",
    "\n",
    "### Random forests <a class=\"anchor\" id=\"random-forests\"></a>\n",
    "\n",
    "The classical decision tree algorithms suggest to grow a tree by splitting each node according to the attribute and the split value that jointly minimize a\n",
    "given cost function. The most commonly used cost functions are the impurity (also called GINI) and the Shannon entropy.\n",
    "It has been shown that in practice such method may lead to overfitting problems. Instead, the model introduced by [Leo Breiman](#leo-breiman) tends to decrease its generalization error by both combining predictions of independent tree classifiers and maintaining a good individual \n",
    "performance for each of them. The random forest algorithm grows individual trees while respecting several properties:\n",
    "\n",
    "* **Random feature selection**: when splitting a node into two or more children, one must considerate a random subset of attributes used to evaluate the cost function, instead of testing each one of them. The subset size is a hyper-parameter that has to be decided empirically.\n",
    "* **Bagging**: Bagging (or bootstrap aggregation) is the fact of fitting each tree of the forest with a subset of the original training set with replacement. Random forests use bagging to both minimize the generalization error (this is possible by jointly randomizing feature selection and using bootstrap aggregation/bagging) and making **out-of-bag estimates** for the generalization error. In order to make out-of-bag estimates, a new classifier (called out-of-bag classifier) is build by combining, for each training instance $X_i$, the predictions made by the trees that did not have $X_i$ in their bagging samples. Hence, only a subset of the trees are used when computing the error related to one instance.\n",
    "* Grown trees are **not pruned** using any post-pruning algorithm.\n",
    "\n",
    "#### VC-dimension of random forests <a class=\"anchor\" id=\"random-forests-vc\"></a>\n",
    "\n",
    "Let's denote $s$ the strength of the set of classifiers $\\{ h(x, \\Theta) \\}$, where $h$ is the random forest prediction function,\n",
    "$x$ is an input instance, and $\\Theta$ is the set of parameters of the model (node children, attribute identifiers and split values).\n",
    "\n",
    "\\begin{equation}\n",
    "  s = \\mathop{\\mathbb{E}}_{X, Y} P_{\\Theta}(h(X, \\Theta) = Y) - \\max_{j \\neq Y} P_{\\Theta}(h(X, \\Theta) = j)\n",
    "\\end{equation}\n",
    "\n",
    "The raw margin function is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "  rmg(\\Theta, X, Y) = argmax_{j \\neq Y} P_{\\Theta} (h(X, \\Theta) = j)\n",
    "\\end{equation}\n",
    "\n",
    "$\\bar{\\rho}$ is the mean value of the correlation $\\rho(\\Theta, \\Theta')$ between $rmg(\\Theta, X, Y)$ and $rmg(\\Theta', X, Y)$ \n",
    "holding $\\Theta, \\Theta'$ fixed.\n",
    "\n",
    "An upper bound for the generalization error is given by the following inequality:\n",
    "\n",
    "\\begin{equation}\n",
    "  PE^{*} \\le \\bar{\\rho} (1 - s^2) / s^2\n",
    "\\end{equation}\n",
    "\n",
    "<span style=\"color:red\">TODO: explanations about the formulas, explanation on how to use oob score to estimate the \n",
    "generalization error and some python code to compute a bound down below, eventually.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model 1 on HandStart...\n",
      "\tROC AUC score: 0.540296\n",
      "\tOut-of-bag error: 0.005000\n",
      "\n",
      "Fit model 2 on FirstDigitTouch...\n",
      "\tROC AUC score: 0.659864\n",
      "\tOut-of-bag error: 0.045000\n",
      "\n",
      "Fit model 3 on BothStartLoadPhase...\n",
      "\tROC AUC score: 0.662162\n",
      "\tOut-of-bag error: 0.040000\n",
      "\n",
      "Fit model 4 on LiftOff...\n",
      "\tROC AUC score: 0.397891\n",
      "\tOut-of-bag error: 0.035000\n",
      "\n",
      "Fit model 5 on Replace...\n",
      "\tROC AUC score: 0.551948\n",
      "\tOut-of-bag error: 0.025000\n",
      "\n",
      "Fit model 6 on BothReleased...\n",
      "\tROC AUC score: 0.764610\n",
      "\tOut-of-bag error: 0.015000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    rf = RandomForestClassifier(n_estimators=150, criterion=\"entropy\", oob_score=True)\n",
    "    rf.fit(X_train, y_train[:, i])\n",
    "    scores = rf.predict_proba(X_val)[:, 1]\n",
    "    oob_error = 1. - rf.oob_score_\n",
    "    print(\"Fit model %i on %s...\" % (i+1, EVENT_NAMES[i]))\n",
    "    print(\"\\tROC AUC score: %f\" % roc_auc_score(y_val[:, i], scores))\n",
    "    print(\"\\tOut-of-bag error: %f\\n\" % oob_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest pruning <a class=\"anchor\" id=\"random-forest-pruning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEAF_NODE = -1\n",
    "\n",
    "def post_pruning(tree, min_samples_leaf=1):\n",
    "    if tree.min_samples_leaf < min_samples_leaf:\n",
    "        tree.min_samples_leaf = min_samples_leaf\n",
    "        tree_ = tree.tree_\n",
    "        for i in range(tree_.node_count):\n",
    "            n_samples = tree_.n_node_samples[i]\n",
    "            if n_samples <= min_samples_leaf:\n",
    "                tree_.children_left[i] = LEAF_NODE\n",
    "                tree_.children_right[i] = LEAF_NODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression <a class=\"anchor\" id=\"logistic-regression\"></a>\n",
    "\n",
    "Logistic regression is a linear model used for classification. Contrary to a linear classifier obtained with the \n",
    "Perceptron Learning Algorithm (PLA), it does not output a binary decision given an instance but real-valued predictions that can be viewed as its degree of confidence that the instance belongs to the positive class. These real values are comprised between\n",
    "0 and 1 and be interpreted as the probability, for each instance, of belonging to the positive class.\n",
    "Like in linear regression, the input vector $x_i$ of instance $i$ is projected to a scalar $s_i$ called the \"signal\".\n",
    "\n",
    "\\begin{equation}\n",
    "  s_i = w^T x_i + c\n",
    "\\end{equation}\n",
    "\n",
    "where c is the intercept. \n",
    "However, nothing ensures that $s_i$ is in $[0, 1]$. Thus, a bounded function called the sigmoid function is \n",
    "applied to $s_i$ to fulfill this goal:\n",
    "\n",
    "\\begin{equation}\n",
    "  h(x_i) = \\sigma(s_i) = \\frac{1}{1 + e^{-w^T x_i + c}} = \\frac{e^{w^T x_i + c}}{1 + e^{w^T x_i + c}}\n",
    "\\end{equation}\n",
    "\n",
    "This is more convenient because in most machine learning application one wishes to get a warning of the model about prediction\n",
    "uncertainty. The model ability to make output continuous intermediate values between a 100%-confident positive prediction\n",
    "and a 100%-confident negative prediction is referred to as soft thresholding.\n",
    "\n",
    "Let's use the default optimization algorithm from scikit-learn to fit our logistic regressor. The parameters of the model\n",
    "are penalized by a L2-regularization. Thus, the optimization problem reduces to minimizing the following cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "  min_w L(w) = min_w \\frac{1}{2} w^T w + C \\cdot \\Sigma_{i=1}^n \\log(e^{-y_i (x_i^T w + c)} + 1)\n",
    "\\end{equation}\n",
    "\n",
    "where parameter $C$ is set to 1 by default, $y_i$ is the ground-truth label $\\in \\{-1, 1\\}$ associated to instance $i$,\n",
    "and $\\frac{1}{2} w^T w$ is the regularization term. This function is better suited for our model than accuracy because it\n",
    "does take the model uncertainty into account when assigning a loss on a particular instance. The default scikit-learn\n",
    "optimization algorithm for logistic regression is based on Coordinate Descent (CD). CD can be described in a few steps:\n",
    "\n",
    "> **1.** Choose an initial weight vector $w^0$. Let $w_i^0$ be the ith component of $w^0$.\n",
    "\n",
    "> **2.** Repeat until stop condition is met\n",
    "\n",
    ">>> **3.** Repeat with $i \\in \\{1, \\ldots, n\\}$\n",
    "\n",
    ">>> **4.** $w_i^{k+1} = argmin_{y} \\ L(w_1^{k+1}, \\ldots, w_{i-1}^{k+1}, y, w_{i+1}^{k}, \\ldots, w_{n}^{k})$\n",
    "\n",
    ">>>This corresponds to a line search along axis $i$.\n",
    "\n",
    ">> **4.** $k \\leftarrow k+1$\n",
    "\n",
    "In brief, we observe that the algorithm optimizes the cost function according to a single dimension at a time, by performing\n",
    "a line search along it. This requires, in average, much more step than gradient descent or quasi-newtonian methods since\n",
    "CD has no information about the direction of the steepest descent.\n",
    "\n",
    "#### VC dimension of logistic regression <a class=\"anchor\" id=\"logistic-regression-vc\"></a>\n",
    "\n",
    "<span style=\"color:red\">TODO: I guess that the VC dimension of logreg is the same as the VC dimension\n",
    "of the linear classifier since they both shatter the data points in the same way, but the VC bounds must be\n",
    "different since the error metric is different -> do some research.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    logreg = LogisticRegression(C= 1., fit_intercept=True)\n",
    "    logreg.fit(X_train, y_train[:, i])\n",
    "    proba = logreg.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract covariance matrices <a class=\"anchor\" id=\"extract-cov\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as cnp\n",
    "cnp.import_array()\n",
    "import cython\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.overflowcheck(False)\n",
    "def extract_cov_matrices(cnp.float_t[:, :] data, Py_ssize_t w):\n",
    "    cdef Py_ssize_t n_features = data.shape[1]\n",
    "    cdef cnp.float_t[:, :, :] sigmas = np.empty((data.shape[0], n_features, n_features), dtype=np.float)\n",
    "    cdef cnp.float_t[:] means = np.asarray(np.mean(np.asarray(data)[:w, :], axis=0), dtype=np.float)\n",
    "    cdef cnp.float_t[:] old_means = np.copy(means)\n",
    "    cdef cnp.float_t[:, :] last_sigma = np.cov(np.asarray(data)[:w, :].T)\n",
    "    cdef cnp.float_t c\n",
    "    np_sigmas = np.asarray(sigmas)\n",
    "    np_sigmas[:w, :, :] = np.repeat(np.asarray(last_sigma).reshape((1, n_features, n_features), order='C'), w, axis=0)\n",
    "    cdef Py_ssize_t i, j, a, b\n",
    "    with nogil:\n",
    "        for i in range(w, data.shape[0]):\n",
    "            for a in range(n_features):\n",
    "                old_means[a] = means[a]\n",
    "                means[a] += (data[i, a] - data[i-w, a]) / w\n",
    "            for a in range(n_features):\n",
    "                for b in range(a+1):\n",
    "                    c = sigmas[i-1, a, b]\n",
    "                    c += (data[i, a] * data[i, b] - data[i-w, a] * data[i-w, b]) / w\n",
    "                    c += old_means[a] * old_means[b] - means[a] * means[b]\n",
    "                    sigmas[i, a, b] = sigmas[i, b, a] = c\n",
    "    return np_sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riemannian-based kernel trick <a class=\"anchor\" id=\"kernel-trick\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hlmap(Cp, sqrtCinv):\n",
    "    return scipy.linalg.logm(sqrtCinv * Cp * sqrtCinv)\n",
    "\n",
    "def hemap(Cp, sqrtC):\n",
    "    return scipy.linalg.expm(sqrtC * Cp * sqrtC)\n",
    "\n",
    "def project_cov_matrices(X):\n",
    "    \"\"\" X is of shape (n_samples, n_electrodes, n_electrodes) \"\"\"\n",
    "    sqrtC = scipy.linalg.sqrtm(X.mean(axis=0))\n",
    "    sqrtCinv = scipy.linalg.inv(sqrtC)\n",
    "    return np.asarray([hlmap(h, sqrtCinv) for h in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riemannian-based Support Vector Machine <a class=\"anchor\" id=\"svm\"></a>\n",
    "\n",
    "<span style=\"color:red\">TODO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis <a class=\"anchor\" id=\"lda\"></a>\n",
    "\n",
    "<span style=\"color:red\">TODO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with non-stationary processes <a class=\"anchor\" id=\"non-stationary-data\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Hidden Markov Models (G-HMM) <a class=\"anchor\" id=\"ghmm\"></a>\n",
    "\n",
    "<span style=\"color:red\">TODO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography <a class=\"anchor\" id=\"bibliography\"></a>\n",
    "<hr/>\n",
    "\n",
    "* <span class=\"anchor\" id=\"bib-riemann\">\n",
    "    [1] Classification of covariance matrices using a Riemannian-based kernel for BCI applications <br>\n",
    "    Alexandre Barachant, Stéphane Bonnet, Marco Congedo, Christian Jutten <br>\n",
    "    https://hal.archives-ouvertes.fr/file/index/docid/820475/filename/BARACHANT_Neurocomputing_ForHal.pdf <br>\n",
    "  </span>\n",
    "<br>\n",
    "\n",
    "* <span class=\"anchor\" id=\"bib-kaggle\">\n",
    "    [2] Grasp-and-Lift EEG Detection Kaggle Competition <br>\n",
    "    https://www.kaggle.com/c/grasp-and-lift-eeg-detection <br>\n",
    "  </span>\n",
    "<br>\n",
    "\n",
    "* <span class=\"anchor\" id=\"leo-breiman\">\n",
    "    [3] Random Forests <br>\n",
    "    Leo Breiman, 2001 <br>\n",
    "    https://link.springer.com/content/pdf/10.1023%2FA%3A1010933404324.pdf <br>\n",
    "  </span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
