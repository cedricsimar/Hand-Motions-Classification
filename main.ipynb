{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand motions classification using non-invasive EEG recordings\n",
    "### by Cedric Simar and Antoine Passemiers\n",
    "<hr/>\n",
    "\n",
    "## Table of content\n",
    "\n",
    "* [0 - Introduction](#introduction)\n",
    "  * [0.1. Problem description](#problem-description)\n",
    "\n",
    "\n",
    "* [1 - Preprocessing](#preprocessing)\n",
    "  * [1.1. Import useful libraries](#import-libraries)\n",
    "  * [1.2. Load the data](#load-data)\n",
    "  * [1.3. Low-pass filtering](#low-pass-filtering)\n",
    "  * [1.4. Downsampling](#downsampling)\n",
    "\n",
    "\n",
    "* [2 - A first naive approach: learn from raw EEG samples](#learn-from-raw-data)\n",
    "  * [2.1. Random Forests (first model)](#random-forests)\n",
    "    * [2.1.1 VC-dimension of random forests](#random-forests-vc)\n",
    "    * [2.1.2 Random forest pruning](#random-forest-pruning)\n",
    "  * [2.2  Logistic regression (second model)](#logistic-regression)\n",
    "    * [2.2.1 VC dimension of logistic regression](#logistic-regression-vc)\n",
    "\n",
    "\n",
    "* [3. Riemannian-based kernel trick](#kernel-trick)\n",
    "  * [3.1. Riemannian-based SVM (fourth model)](#ghmm)\n",
    "  * [3.2. Linear Discriminant Analysis (fifth model)](#lda)\n",
    "\n",
    "\n",
    "* [4 - Dealing with non-stationary data](#non-stationary-data)\n",
    "  * [4.1. Gaussian Hidden Markov Models (third model)](#ghmm)\n",
    "\n",
    "\n",
    "* [5. ANN, LSTM and weird stuff](#CAFE)\n",
    "\n",
    "\n",
    "* [X. Bibliography](#bibliography)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "<hr/>\n",
    "\n",
    "Note: This notebook has been designed in order to serve several purposes:\n",
    "1. Introduce different models either that have been successfully applied to EEG-based problems in the past or that are considered by us as being relevant given the nature of the task.\n",
    "2. Constantly refer to generalization theory while selecting our best models using Structural Risk Minimization (SRM)\n",
    "3. Present code excerpts used to achieve good performance on the [Kaggle competition](#bib-kaggle) about Grasp-and-Lift EEG detection.\n",
    "\n",
    "### Problem description <a class=\"anchor\" id=\"problem-description\"></a>\n",
    "\n",
    "A Brain-Computer Interface (BCI) is a system that establishes a direct communication between a brain and a device without using the neuromuscular pathways. The design and implementation of a BCI system typically require a multidisciplinary approach in order to combine expertise from different fields such as neurophysiology, electronics, signal processing and algorithmics.\n",
    "\n",
    "Currently, the main field of application of BCI systems is the medical domain. Each year, thousands of patients suffer from severe motor paralysis caused by a trauma or a generative disease. The research objective of the [Kaggle competition](#bib-kaggle) is to develop accurate and robust algorithms that can be integrated in BCI systems to considerably enhance the quality of life of these patients by allowing them to regain some control over their environment. More specifaclly the competition focus on classifying hand motions of 6 different movements: 'HandStart', 'FirstDigitTouch', 'BothStartLoadPhase', 'LiftOff', 'Replace', and 'BothReleased'. With the help of Machine Learning classification algorithms integrated in a BCI system, the hope is to be able to map brain waves to the desired movement and, eventually control a robotic arm or a prosthesis capable of playing the role of a hand.\n",
    "\n",
    "### EEG analysis <a class=\"anchor\" id=\"problem-description\"></a>\n",
    "\n",
    "An electroencephalogram (EEG) uses multiple electrodes to measure the electrical activity of post-synaptic potentials of cortical neurons located at specific parts of the brain. These electrodes are placed at precise locations on the patient' scalp following the International 10-20 system as illustrated hereunder. \n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"imgs/EEG_Electrode_Numbering.jpg\" style=\"width:450px;\">\n",
    "  <figcaption> Source: [Kaggle](#bib-kaggle) </figcaption>\n",
    "</figure>\n",
    "\n",
    "The purpose of an EEG signal analysis is to use advanced signal processing techniques to extract relevant information about the brain state that are not directly visible in the time domain for the diagnosis of brain-related pathologies. The frequency domain analysis implemented in this work is a signal processing technique that divides the signal spectrum in different frequency bands, each associated with different brain states and mental activities.\n",
    "\n",
    "### Data <a class=\"anchor\" id=\"problem-description\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe style=\"display:block\" width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/y3_Izuop2gY?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "url = \"https://www.youtube.com/embed/y3_Izuop2gY?rel=0&amp;controls=0&amp;showinfo=0\"\n",
    "HTML('<iframe style=\"display:block\" width=\"560\" height=\"315\" src=\"%s\" frameborder=\"0\" allowfullscreen></iframe>' % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "<hr/>\n",
    "\n",
    "### Import useful libraries <a class=\"anchor\" id=\"import-libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython\n",
    "\n",
    "import cython\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import scipy.linalg\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data <a class=\"anchor\" id=\"load-data\"></a>\n",
    "\n",
    "TODO Cedric: descriptions (electrodes, multi-labels, échantillonage, tout ça tout ça)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_PATIENTS = 12\n",
    "# Feature names\n",
    "ELECTRODE_NAMES = [\n",
    "    'Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'FC5', 'FC1', 'FC2', 'FC6',\n",
    "    'T7', 'C3', 'Cz', 'C4', 'T8', 'TP9', 'CP5', 'CP1', 'CP2', 'CP6', 'TP10',\n",
    "    'P7', 'P3', 'Pz', 'P4', 'P8', 'PO9', 'O1', 'Oz', 'O2', 'PO10']\n",
    "# Label names\n",
    "EVENT_NAMES = ['HandStart', 'FirstDigitTouch', 'BothStartLoadPhase', 'LiftOff', 'Replace', 'BothReleased']\n",
    "\n",
    "\n",
    "def load_dataset(subject=1, data_dir='', series=range(1,9)):\n",
    "    data, events = list(), list()\n",
    "    for series_id, s in enumerate(series):\n",
    "        print(\"Load series {0} from patient {1}\".format(series_id+1, subject))\n",
    "        data_filepath = os.path.join(data_dir, \"train/subj{0}_series{1}_data.csv\".format(subject, s))\n",
    "        data.append(pd.read_csv(data_filepath, index_col=0).values.astype(np.float))\n",
    "        events_filepath = os.path.join(data_dir, \"train/subj{0}_series{1}_events.csv\".format(subject, s))\n",
    "        events.append(pd.read_csv(events_filepath, index_col=0).values)\n",
    "    return data, np.concatenate(events, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load series 1 from patient 1\n",
      "Load series 2 from patient 1\n",
      "Load series 3 from patient 1\n",
      "Load series 4 from patient 1\n",
      "Load series 5 from patient 1\n",
      "Load series 6 from patient 1\n",
      "Load series 7 from patient 1\n",
      "Load series 8 from patient 1\n"
     ]
    }
   ],
   "source": [
    "data, events = load_dataset(subject=1, data_dir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-pass filtering <a class=\"anchor\" id=\"low-pass-filtering\"></a>\n",
    "\n",
    "To avoid creating potential aliasing effects, we apply necessary spectral filters on the raw signals before to downsample them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LowPassFilter:\n",
    "    \n",
    "    def __init__(self, cutoff_freq, sampling_freq, order=3):\n",
    "        self.nyquist_freq = sampling_freq / 2.\n",
    "        bound = cutoff_freq / self.nyquist_freq\n",
    "        self.b, self.a = scipy.signal.butter(order, bound, btype='lowpass', analog=True)\n",
    "    \n",
    "    def filter(self, signal):\n",
    "        # TODO: does lfiter include future data when filtering current frame?\n",
    "        return scipy.signal.lfilter(self.b, self.a, signal, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fp1 = [series[:, 0] for series in data] # TODO: electrode 0 or 15\n",
    "filtered = list()\n",
    "for cutoff_freq in np.linspace(0, 1, 11)[1:]:\n",
    "    lowpass = LowPassFilter(cutoff_freq, 500)\n",
    "    filtered.append(np.concatenate([lowpass.filter(series_fp1) for series_fp1 in Fp1], axis=0)[:, np.newaxis])\n",
    "filtered = np.concatenate(filtered, axis=1)\n",
    "X_train_raw = np.concatenate((np.concatenate(data, axis=0), filtered, filtered ** 2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling <a class=\"anchor\" id=\"downsampling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_downsampled = X_train_raw[::4000]\n",
    "y_downsampled = events[::4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_downsampled = StandardScaler(copy=False).fit_transform(X_train_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val = X_train_downsampled[:200], X_train_downsampled[200:]\n",
    "y_train, y_val = y_downsampled[:200], y_downsampled[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first naive approach: learn from EEG samples <a class=\"anchor\" id=\"learn-from-raw-data\"></a>\n",
    "<hr/>\n",
    "\n",
    "### Random forests <a class=\"anchor\" id=\"random-forests\"></a>\n",
    "\n",
    "The classical decision tree algorithms suggest to grow a tree by splitting each node according to the attribute and the split value that jointly minimize a\n",
    "given cost function. The most commonly used cost functions are the impurity (also called GINI) and the Shannon entropy.\n",
    "It has been shown that in practice such method may lead to overfitting problems. Instead, the model introduced by [Leo Breiman](#leo-breiman) tends to decrease its generalization error by both combining predictions of independent tree classifiers and maintaining a good individual \n",
    "performance for each of them. The random forest algorithm grows individual trees while respecting several properties:\n",
    "\n",
    "* **Random feature selection**: when splitting a node into two or more children, one must considerate a random subset of attributes used to evaluate the cost function, instead of testing each one of them. The subset size is a hyper-parameter that has to be decided empirically.\n",
    "* **Bagging**: Bagging (or bootstrap aggregation) is the fact of fitting each tree of the forest with a subset of the original training set with replacement. Random forests use bagging to both minimize the generalization error (this is possible by jointly randomizing feature selection and using bootstrap aggregation/bagging) and making **out-of-bag estimates** for the generalization error. In order to make out-of-bag estimates, a new classifier (called out-of-bag classifier) is build by combining, for each training instance $X_i$, the predictions made by the trees that did not have $X_i$ in their bagging samples. Hence, only a subset of the trees are used when computing the error related to one instance.\n",
    "* Grown trees are **not pruned** using any post-pruning algorithm.\n",
    "\n",
    "#### VC-dimension of random forests <a class=\"anchor\" id=\"random-forests-vc\"></a>\n",
    "\n",
    "Let's denote $s$ the strength of the set of classifiers $\\{ h(x, \\Theta) \\}$, where $h$ is the random forest prediction function,\n",
    "$x$ is an input instance, and $\\Theta$ is the set of parameters of the model (node children, attribute identifiers and split values).\n",
    "\n",
    "\\begin{equation}\n",
    "  s = \\mathop{\\mathbb{E}}_{X, Y} P_{\\Theta}(h(X, \\Theta) = Y) - \\max_{j \\neq Y} P_{\\Theta}(h(X, \\Theta) = j)\n",
    "\\end{equation}\n",
    "\n",
    "The raw margin function is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "  rmg(\\Theta, X, Y) = argmax_{j \\neq Y} P_{\\Theta} (h(X, \\Theta) = j)\n",
    "\\end{equation}\n",
    "\n",
    "$\\bar{\\rho}$ is the mean value of the correlation $\\rho(\\Theta, \\Theta')$ between $rmg(\\Theta, X, Y)$ and $rmg(\\Theta', X, Y)$ \n",
    "holding $\\Theta, \\Theta'$ fixed.\n",
    "\n",
    "An upper bound for the generalization error is given by the following inequality:\n",
    "\n",
    "\\begin{equation}\n",
    "  PE^{*} \\le \\bar{\\rho} (1 - s^2) / s^2\n",
    "\\end{equation}\n",
    "\n",
    "<span style=\"color:red\">TODO: explanations about the formulas, explanation on how to use oob score to estimate the \n",
    "generalization error and some python code to compute a bound down below, eventually.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model 1 on HandStart...\n",
      "\tROC AUC score: 0.540296\n",
      "\tOut-of-bag error: 0.005000\n",
      "\n",
      "Fit model 2 on FirstDigitTouch...\n",
      "\tROC AUC score: 0.659864\n",
      "\tOut-of-bag error: 0.045000\n",
      "\n",
      "Fit model 3 on BothStartLoadPhase...\n",
      "\tROC AUC score: 0.662162\n",
      "\tOut-of-bag error: 0.040000\n",
      "\n",
      "Fit model 4 on LiftOff...\n",
      "\tROC AUC score: 0.397891\n",
      "\tOut-of-bag error: 0.035000\n",
      "\n",
      "Fit model 5 on Replace...\n",
      "\tROC AUC score: 0.551948\n",
      "\tOut-of-bag error: 0.025000\n",
      "\n",
      "Fit model 6 on BothReleased...\n",
      "\tROC AUC score: 0.764610\n",
      "\tOut-of-bag error: 0.015000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    rf = RandomForestClassifier(n_estimators=150, criterion=\"entropy\", oob_score=True)\n",
    "    rf.fit(X_train, y_train[:, i])\n",
    "    scores = rf.predict_proba(X_val)[:, 1]\n",
    "    oob_error = 1. - rf.oob_score_\n",
    "    print(\"Fit model %i on %s...\" % (i+1, EVENT_NAMES[i]))\n",
    "    print(\"\\tROC AUC score: %f\" % roc_auc_score(y_val[:, i], scores))\n",
    "    print(\"\\tOut-of-bag error: %f\\n\" % oob_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest pruning <a class=\"anchor\" id=\"random-forest-pruning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEAF_NODE = -1\n",
    "\n",
    "def post_pruning(tree, min_samples_leaf=1):\n",
    "    if tree.min_samples_leaf < min_samples_leaf:\n",
    "        tree.min_samples_leaf = min_samples_leaf\n",
    "        tree_ = tree.tree_\n",
    "        for i in range(tree_.node_count):\n",
    "            n_samples = tree_.n_node_samples[i]\n",
    "            if n_samples <= min_samples_leaf:\n",
    "                tree_.children_left[i] = LEAF_NODE\n",
    "                tree_.children_right[i] = LEAF_NODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression <a class=\"anchor\" id=\"logistic-regression\"></a>\n",
    "\n",
    "Logistic regression is a linear model used for classification. Contrary to a linear classifier obtained with the \n",
    "Perceptron Learning Algorithm (PLA), it does not output a binary decision given an instance but real-valued predictions that can be viewed as its degree of confidence that the instance belongs to the positive class. These real values are comprised between\n",
    "0 and 1 and be interpreted as the probability, for each instance, of belonging to the positive class.\n",
    "Like in linear regression, the input vector $x_i$ of instance $i$ is projected to a scalar $s_i$ called the \"signal\".\n",
    "\n",
    "\\begin{equation}\n",
    "  s_i = w^T x_i + c\n",
    "\\end{equation}\n",
    "\n",
    "where c is the intercept. \n",
    "However, nothing ensures that $s_i$ is in $[0, 1]$. Thus, a bounded function called the sigmoid function is \n",
    "applied to $s_i$ to fulfill this goal:\n",
    "\n",
    "\\begin{equation}\n",
    "  h(x_i) = \\sigma(s_i) = \\frac{1}{1 + e^{-w^T x_i + c}} = \\frac{e^{w^T x_i + c}}{1 + e^{w^T x_i + c}}\n",
    "\\end{equation}\n",
    "\n",
    "This is more convenient because in most machine learning application one wishes to get a warning of the model about prediction\n",
    "uncertainty. The model ability to make output continuous intermediate values between a 100%-confident positive prediction\n",
    "and a 100%-confident negative prediction is referred to as soft thresholding.\n",
    "\n",
    "Let's use the default optimization algorithm from scikit-learn to fit our logistic regressor. The parameters of the model\n",
    "are penalized by a L2-regularization. Thus, the optimization problem reduces to minimizing the following cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "  min_w L(w) = min_w \\frac{1}{2} w^T w + C \\cdot \\Sigma_{i=1}^n \\log(e^{-y_i (x_i^T w + c)} + 1)\n",
    "\\end{equation}\n",
    "\n",
    "where parameter $C$ is set to 1 by default, $y_i$ is the ground-truth label $\\in \\{-1, 1\\}$ associated to instance $i$,\n",
    "and $\\frac{1}{2} w^T w$ is the regularization term. This function is better suited for our model than accuracy because it\n",
    "does take the model uncertainty into account when assigning a loss on a particular instance. The default scikit-learn\n",
    "optimization algorithm for logistic regression is based on Coordinate Descent (CD). CD can be described in a few steps:\n",
    "\n",
    "> **1.** Choose an initial weight vector $w^0$. Let $w_i^0$ be the ith component of $w^0$.\n",
    "\n",
    "> **2.** Repeat until stop condition is met\n",
    "\n",
    ">>> **3.** Repeat with $i \\in \\{1, \\ldots, n\\}$\n",
    "\n",
    ">>> **4.** $w_i^{k+1} = argmin_{y} \\ L(w_1^{k+1}, \\ldots, w_{i-1}^{k+1}, y, w_{i+1}^{k}, \\ldots, w_{n}^{k})$\n",
    "\n",
    ">>>This corresponds to a line search along axis $i$.\n",
    "\n",
    ">> **4.** $k \\leftarrow k+1$\n",
    "\n",
    "In brief, we observe that the algorithm optimizes the cost function according to a single dimension at a time, by performing\n",
    "a line search along it. This requires, in average, much more step than gradient descent or quasi-newtonian methods since\n",
    "CD has no information about the direction of the steepest descent.\n",
    "\n",
    "#### VC dimension of logistic regression <a class=\"anchor\" id=\"logistic-regression-vc\"></a>\n",
    "\n",
    "<span style=\"color:red\">TODO: I guess that the VC dimension of logreg is the same as the VC dimension\n",
    "of the linear classifier since they both shatter the data points in the same way, but the VC bounds must be\n",
    "different since the error metric is different -> do some research.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    logreg = LogisticRegression(C= 1., fit_intercept=True)\n",
    "    logreg.fit(X_train, y_train[:, i])\n",
    "    proba = logreg.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract covariance matrices <a class=\"anchor\" id=\"extract-cov\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as cnp\n",
    "cnp.import_array()\n",
    "import cython\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.overflowcheck(False)\n",
    "def extract_cov_matrices(cnp.float_t[:, :] data, Py_ssize_t w):\n",
    "    cdef Py_ssize_t n_features = data.shape[1]\n",
    "    cdef cnp.float_t[:, :, :] sigmas = np.empty((data.shape[0], n_features, n_features), dtype=np.float)\n",
    "    cdef cnp.float_t[:] means = np.asarray(np.mean(np.asarray(data)[:w, :], axis=0), dtype=np.float)\n",
    "    cdef cnp.float_t[:] old_means = np.copy(means)\n",
    "    cdef cnp.float_t[:, :] last_sigma = np.cov(np.asarray(data)[:w, :].T)\n",
    "    cdef cnp.float_t c\n",
    "    np_sigmas = np.asarray(sigmas)\n",
    "    np_sigmas[:w, :, :] = np.repeat(np.asarray(last_sigma).reshape((1, n_features, n_features), order='C'), w, axis=0)\n",
    "    cdef Py_ssize_t i, j, a, b\n",
    "    with nogil:\n",
    "        for i in range(w, data.shape[0]):\n",
    "            for a in range(n_features):\n",
    "                old_means[a] = means[a]\n",
    "                means[a] += (data[i, a] - data[i-w, a]) / w\n",
    "            for a in range(n_features):\n",
    "                for b in range(a+1):\n",
    "                    c = sigmas[i-1, a, b]\n",
    "                    c += (data[i, a] * data[i, b] - data[i-w, a] * data[i-w, b]) / w\n",
    "                    c += old_means[a] * old_means[b] - means[a] * means[b]\n",
    "                    sigmas[i, a, b] = sigmas[i, b, a] = c\n",
    "    return np_sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riemannian-based kernel trick <a class=\"anchor\" id=\"kernel-trick\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hlmap(Cp, sqrtCinv):\n",
    "    return scipy.linalg.logm(sqrtCinv * Cp * sqrtCinv)\n",
    "\n",
    "def hemap(Cp, sqrtC):\n",
    "    return scipy.linalg.expm(sqrtC * Cp * sqrtC)\n",
    "\n",
    "def project_cov_matrices(X):\n",
    "    \"\"\" X is of shape (n_samples, n_electrodes, n_electrodes) \"\"\"\n",
    "    sqrtC = scipy.linalg.sqrtm(X.mean(axis=0))\n",
    "    sqrtCinv = scipy.linalg.inv(sqrtC)\n",
    "    return np.asarray([hlmap(h, sqrtCinv) for h in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riemannian-based Support Vector Machine <a class=\"anchor\" id=\"svm\"></a>\n",
    "\n",
    "<span style=\"color:red\">TODO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis <a class=\"anchor\" id=\"lda\"></a>\n",
    "\n",
    "<span style=\"color:red\">TODO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with non-stationary processes <a class=\"anchor\" id=\"non-stationary-data\"></a>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Hidden Markov Models (G-HMM) <a class=\"anchor\" id=\"ghmm\"></a>\n",
    "\n",
    "<span style=\"color:red\">TODO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "class Settings:\n",
    "\n",
    "    LOADED_FILE = None\n",
    "\n",
    "    @staticmethod\n",
    "    def add_attr(name, value):\n",
    "        \"\"\" Statically adds a parameter as an attribute\n",
    "        to class Settings. All new Settings attributes\n",
    "        are in capital letters.\n",
    "\n",
    "        :param name: str\n",
    "            Name of the new attribute\n",
    "        :param value: object\n",
    "            Value of the corresponding hyper-parameter\n",
    "        \"\"\"\n",
    "        name = name.upper()\n",
    "        setattr(Settings, name, value)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_attr(name):\n",
    "        return getattr(Settings, name.upper())\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        \"\"\" Statically loads the hyper-Settings from a json file\n",
    "\n",
    "        :param filepath: str\n",
    "            Path to the json parameter file\n",
    "        \"\"\"\n",
    "        Settings.LOADED_FILE = filepath\n",
    "        Settings.TO_UPDATE = list()\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            for key in sorted(data.keys()):\n",
    "                if isinstance(data[key], dict):\n",
    "                    Settings.add_attr(key, data[key][\"value\"])\n",
    "                    \n",
    "    @staticmethod\n",
    "    def update():\n",
    "        if Settings.LOADED_FILE is None:\n",
    "            print('[Warning] Trying to save Settings but none have been loaded.')\n",
    "            return\n",
    "        with open(Settings.LOADED_FILE, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            for key in data:\n",
    "                if not isinstance(data[key], dict) or key not in Settings.TO_UPDATE:\n",
    "                    continue\n",
    "                if data[key]['value'] != Settings.get_attr(key):\n",
    "                    data[key][\"value\"] = Settings.get_attr(key)\n",
    "        with open(Settings.LOADED_FILE, \"w\") as f:\n",
    "            pretty_str = json.dumps(data, indent=4, sort_keys=True)\n",
    "            f.write(pretty_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formating classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Aggregate the data from csv files of all subjects in one .npy file\n",
    "\"\"\"\n",
    "\n",
    "class DataAggregator:\n",
    "\n",
    "    def aggregate(self, csv_data_path, destination_path, purpose = \"train\"):\n",
    "        \n",
    "        assert(purpose in[\"train\", \"test\"])\n",
    "        if(not os.path.exists(destination_path)): os.mkdir(destination_path)\n",
    "        aggregated_file_name = \"aggregated_\" + purpose + \".npy\"\n",
    "        aggregated_file_path = os.path.join(destination_path, aggregated_file_name)\n",
    "\n",
    "        if(os.path.exists(aggregated_file_path)):\n",
    "            print(\"Aggregated data file already exists: overwrite? (y/n) : \", end='')\n",
    "            if(input() == 'n'): return()\n",
    "            \n",
    "        aggregated_data = []\n",
    "        aggregated_labels = []\n",
    "\n",
    "        for subject_id in range(Settings.NUM_SUBJECTS):\n",
    "\n",
    "            print(\"Aggregating subject\", subject_id+1)\n",
    "\n",
    "            subject_data = []\n",
    "            subject_labels = []\n",
    "\n",
    "            for serie_id in range(Settings.NUM_SERIES):\n",
    "\n",
    "                # load and append the data into subject_data\n",
    "\n",
    "                file_name = \"subj\" + str(subject_id + 1) + \"_series\" + str(serie_id + 1) + \"_data.csv\"\n",
    "                file_path = os.path.join(csv_data_path, file_name)\n",
    "                serie_data = pd.read_csv(file_path)\n",
    "\n",
    "                electrodes_names = list(serie_data.columns[1:]) # discard the index column\n",
    "                subject_data.append(np.array(serie_data[electrodes_names], dtype=\"float32\"))\n",
    "\n",
    "                if(purpose == \"train\"):\n",
    "                        \n",
    "                    # load and append the labels into subject_labels\n",
    "\n",
    "                    file_name = \"subj\" + str(subject_id + 1) + \"_series\" + str(serie_id + 1) + \"_events.csv\"\n",
    "                    file_path = os.path.join(csv_data_path, file_name)\n",
    "                    serie_labels = pd.read_csv(file_path)\n",
    "\n",
    "                    events_names = list(serie_labels.columns[1:]) # discard the index column\n",
    "                    subject_labels.append(np.array(serie_labels[events_names], dtype=\"float32\"))\n",
    "            \n",
    "                        \n",
    "            aggregated_data.append(subject_data)\n",
    "            aggregated_labels.append(subject_labels)\n",
    "    \n",
    "\n",
    "        # save the aggregated data and labels\n",
    "        np.save(aggregated_file_path, [aggregated_data, aggregated_labels])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BatchGenerator:\n",
    "\n",
    "    \"\"\"\n",
    "    Generate batches to feed the RCNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "                \n",
    "        # to adapt for the 3Gb file\n",
    "\n",
    "        # load subjects data file(s)\n",
    "        self.data, self.labels = np.load(file_path)\n",
    "\n",
    "        # separate training from validation data\n",
    "        # training data is of shape 6 x 119496 x 32 [series][signal][channel]\n",
    "        self.training_data = self.data[Settings.SERIES_TRAINING_INDICES]\n",
    "        self.training_labels = self.labels[Settings.SERIES_TRAINING_INDICES]\n",
    "\n",
    "        self.validation_data = self.data[Settings.SERIES_VALIDATION_INDICES]\n",
    "        self.validation_labels = self.labels[Settings.SERIES_VALIDATION_INDICES]\n",
    "\n",
    "        # get indices of positive and negative data indices in each series\n",
    "        self.positive_training_indices = [np.where(np.sum(self.training_labels[series], axis=1) > 0)[0] for series in range(len(self.training_data))]\n",
    "        self.negative_training_indices = [np.where(np.sum(self.training_labels[series], axis=1) == 0)[0] for series in range(len(self.training_data))]\n",
    "\n",
    "\n",
    "    def get_random_series(self, x):\n",
    "        return(np.random.randint(len(x)))\n",
    "\n",
    "    \n",
    "    def get_random_point(self, series, x):\n",
    "        return(np.random.randint(Settings.SAMPLE_LENGTH - 1, len(x[series])))\n",
    "        \n",
    "\n",
    "    def random_batch(self, purpose, size):\n",
    "\n",
    "        \"\"\"\n",
    "        Generate a batch of fixed size for the training or validation\n",
    "        We also generate a batch for validation since the number of sample doesn't fit in RAM\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "\n",
    "        if purpose == \"training\":\n",
    "            x = self.training_data\n",
    "            y = self.training_labels\n",
    "        else:\n",
    "            x = self.validation_data\n",
    "            y = self.validation_labels\n",
    "\n",
    "        for _ in range(size):\n",
    "\n",
    "            series = self.get_random_series(x)\n",
    "            point = self.get_random_point(series, x)   # time\n",
    "\n",
    "            # sample is of dimension Settings.SAMPLE_LENGTH x 32\n",
    "            sample = np.copy(x[series][point - Settings.SAMPLE_LENGTH + 1:point + 1, :])\n",
    "            \n",
    "            # per sample per channel mean substraction\n",
    "            sample -= np.mean(sample, axis=0).reshape((1, Settings.NUM_CHANNELS))\n",
    "\n",
    "            # reshape sample to 32 x Settings.SAMPLE_LENGTH\n",
    "            sample = sample.T\n",
    "\n",
    "            batch_data.append(sample)\n",
    "            batch_labels.append(y[series][point])\n",
    "\n",
    "        return((np.asarray(batch_data), np.asarray(batch_labels)))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorator functions for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator decorator, allowing to use the decorator to be used without\n",
    "    parentheses if no argument is provided. All arguments must be optional.\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    A decorator for functions that define TensorFlow operations. The wrapped\n",
    "    function will only be executed once. Subsequent calls to it will directly\n",
    "    return the result so that operations are added to the graph only once.\n",
    "    The operations added by the function live within a tf.variable_scope(). If\n",
    "    this decorator is used with arguments, they will be forwarded to the\n",
    "    variable scope. The scope name defaults to the name of the wrapped\n",
    "    function.\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops as tf_array_ops\n",
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer\n",
    "\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, input_ph, labels_ph):\n",
    "\n",
    "        \"\"\"\n",
    "        Simple CNN approach (no recurrence)\n",
    "        \"\"\"\n",
    "\n",
    "        # receiving input placeholder\n",
    "        self.input = input_ph\n",
    "        self.labels = labels_ph\n",
    "\n",
    "        # Network settings\n",
    "        self.norm_eps = Settings.NORMALIZATION_EPSILON\n",
    "\n",
    "        # getting the number of hand motions to classify\n",
    "        self.output_size = Settings.NUM_EVENTS\n",
    "\n",
    "        # weights and biases dictionary\n",
    "        self.learning_parameters = {}\n",
    "        self.layers = {}\n",
    "\n",
    "        # initialize tensorflow graph\n",
    "        self.predict\n",
    "        self.optimize\n",
    "        self.loss\n",
    "\n",
    "        # Initialize input placeholder to assign values to weights and biases\n",
    "        with tf.variable_scope(\"input_assignment\"):\n",
    "\n",
    "            self.l_param_input = {}\n",
    "            self.assign_operator = {}\n",
    "            for variable_name in self.learning_parameters.keys():\n",
    "                self.l_param_input[variable_name] = tf.placeholder(\n",
    "                    tf.float32,\n",
    "                    self.learning_parameters[variable_name].get_shape().as_list(),\n",
    "                    name=variable_name)\n",
    "\n",
    "                try:  # If mutable tensor (Variable)\n",
    "                    self.assign_operator[variable_name] = self.learning_parameters[variable_name].assign(\n",
    "                        self.l_param_input[variable_name])\n",
    "                except AttributeError as e:\n",
    "                    print(e)\n",
    "\n",
    "\n",
    "    @define_scope\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        The input to the neural network consists of a 32 channels x SAMPLE_LENGTH signal \n",
    "        produced by the preprocessing stage\n",
    "        \"\"\"\n",
    "        # reshape input to 3d tensor [batch, channels, sample length]\n",
    "        input_layer = tf.reshape(self.input,\n",
    "                                [-1, Settings.NUM_CHANNELS, Settings.SAMPLE_LENGTH, 1])\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 1: 1D spatial convolution over the channels to condense NUM_CHANNELS to\n",
    "                 the number of the convolutional filters (thus reducing the input dimenstion)\n",
    "                 Linear transformation, no activation function used\n",
    "        \"\"\"\n",
    "        conv1 = tf.layers.conv2d(inputs=input_layer, filters=4, kernel_size=[Settings.NUM_CHANNELS, 1],\n",
    "                                 strides = (1, 1), padding=\"valid\", kernel_initializer=glorot_uniform_initializer(),\n",
    "                                 activation=None)\n",
    "\n",
    "        print(\"conv1 shape: \", conv1.get_shape())\n",
    "                                 \n",
    "        \"\"\"\n",
    "        Layer 2: 1D temporal convolution over the raw signal of each channel\n",
    "                 Activation function: ReLu\n",
    "                 Max pooling \n",
    "        \"\"\"\n",
    "        conv2 = tf.layers.conv2d(inputs=conv1, filters=32, kernel_size=[1, 9], strides=(1, 1),\n",
    "                                 padding = \"same\", kernel_initializer = glorot_uniform_initializer(),\n",
    "                                 activation = tf.nn.leaky_relu)\n",
    "\n",
    "        print(\"conv2 shape: \", conv2.get_shape())\n",
    "\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(1, 4), strides=(1, 4))\n",
    "        # no dropout in the first layer\n",
    "\n",
    "        print(\"pool2 shape: \", pool2.get_shape())\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 3: Second 1D temporal convolution\n",
    "                 Activation function: ReLu\n",
    "                 Max pooling\n",
    "                 Flatten the output of the max pooling layer to get the input of the FC layer\n",
    "                 Dropout layer with p = 0.5\n",
    "        \"\"\"\n",
    "        conv3 = tf.layers.conv2d(inputs=pool2, filters=32, kernel_size=[1, 9], strides=(1, 1),\n",
    "                                 padding = \"same\", kernel_initializer = glorot_uniform_initializer(),\n",
    "                                 activation = tf.nn.leaky_relu)\n",
    "        \n",
    "        pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=(1, 4), strides=(1, 4))\n",
    "\n",
    "        flat3 = tf.layers.flatten(pool3)\n",
    "        \n",
    "        drop3 = tf.layers.dropout(inputs=flat3, rate=Settings.DROPOUT_RATE)\n",
    "        \n",
    "\n",
    "        print(\"drop3 shape: \", drop3.get_shape())\n",
    "\n",
    "        \"\"\"\n",
    "        Layers 4 and 5: Standard Fully Connected layers separated by Dropout layers\n",
    "        \"\"\"\n",
    "        fc4 = tf.layers.dense(inputs=drop3, units=1024, activation=tf.nn.relu, use_bias=True,\n",
    "                              kernel_initializer = glorot_uniform_initializer())\n",
    "        drop4 = tf.layers.dropout(inputs=fc4, rate=Settings.DROPOUT_RATE)\n",
    "\n",
    "        fc5 = tf.layers.dense(inputs=drop4, units=1024, activation=tf.nn.relu, use_bias=True,\n",
    "                              kernel_initializer = glorot_uniform_initializer())\n",
    "        drop5 = tf.layers.dropout(inputs=fc5, rate=Settings.DROPOUT_RATE)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Layer 6: Output layer\n",
    "        \"\"\"\n",
    "        logits = tf.layers.dense(inputs=drop5, units=Settings.NUM_EVENTS,\n",
    "                               kernel_initializer = glorot_uniform_initializer(),\n",
    "                               activation = None)\n",
    "\n",
    "        # using sigmoid cross entropy (not mutually exclusive) with logits so no need of an\n",
    "        # activation function at the end of the CNN\n",
    "        \n",
    "        return(logits)\n",
    "\n",
    "\n",
    "    @define_scope\n",
    "    def predict_proba(self):\n",
    "        return(tf.sigmoid(self.predict))\n",
    "    \n",
    "    @define_scope\n",
    "    def accuracy(self):\n",
    "        self.predictions = tf.round(self.predict_proba)\n",
    "        self.correct_predictions = tf.cast(tf.equal(self.predictions, self.labels), tf.float32)\n",
    "        return(tf.reduce_mean(self.correct_predictions))\n",
    "\n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "\n",
    "        self.optimizer = tf.train.MomentumOptimizer(\n",
    "            learning_rate = Settings.LEARNING_RATE,\n",
    "            momentum = Settings.MOMENTUM,\n",
    "            use_nesterov = True\n",
    "        )\n",
    "        return(self.optimizer.minimize(self.loss))\n",
    "\n",
    "\n",
    "    @define_scope\n",
    "    def loss(self):\n",
    "        \"\"\" Return the mean error \"\"\"\n",
    "\n",
    "        self.error = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.labels, logits=self.predict)\n",
    "        self.mean_error = tf.reduce_mean(self.error, name=\"mean_error\")\n",
    "        return(self.mean_error)\n",
    "\n",
    "    \n",
    "    def get_value(self, var_name, tf_session):\n",
    "        \"\"\"\n",
    "        Return the value of the tf variable named [var_name] if it exists, None otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        if var_name in self.learning_parameters:\n",
    "\n",
    "            value = tf_session.run(self.learning_parameters[var_name])\n",
    "\n",
    "        elif var_name in self.layers:\n",
    "\n",
    "            value = tf_session.run(self.layers[var_name])\n",
    "\n",
    "        else:\n",
    "            print(\"Unknown DQN variable: \" + var_name)\n",
    "            assert(0)  # <3\n",
    "\n",
    "        return(value)\n",
    "\n",
    "    def set_value(self, var_name, new_value, tf_session):\n",
    "        \"\"\"\n",
    "        Set the value of the tf variable [var_name] to [new_value]\n",
    "        \"\"\"\n",
    "\n",
    "        if(var_name in self.assign_operator):\n",
    "\n",
    "            tf_session.run(\n",
    "                self.assign_operator[var_name], {\n",
    "                    self.l_param_input[var_name]: new_value})\n",
    "        else:\n",
    "            print(\"Thou shall only assign learning parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# initialize Settings class with json file\n",
    "Settings.load(\"./src/settings.json\")\n",
    "\n",
    "# aggregate data per patient\n",
    "# DataAggregator().aggregate(Settings.TRAIN_DATA_CSV_PATH, Settings.TRAIN_AGG_DATA_PATH, purpose=\"train\")\n",
    "\n",
    "# initialize a batch generator using the aggregated data\n",
    "batchGen = BatchGenerator(\"./data/train/aggregated/sub1_aggregated_train.npy\")\n",
    "\n",
    "# initialize training session variables\n",
    "\n",
    "input_ph = tf.placeholder(tf.float32, [None, Settings.NUM_CHANNELS, Settings.SAMPLE_LENGTH])\n",
    "labels_ph = tf.placeholder(tf.float32, [None, Settings.NUM_EVENTS])\n",
    "\n",
    "nn = CNN(input_ph, labels_ph)\n",
    "\n",
    "tf_session = tf.Session()\n",
    "tf_session.run(tf.global_variables_initializer())\n",
    "\n",
    "# run training session loop\n",
    "for training_iteration in range(1000):\n",
    "\n",
    "    # get next training batch\n",
    "    batch_x, batch_y = batchGen.random_batch(\"training\", Settings.MINIBATCH_SIZE)\n",
    "\n",
    "    # run a training step\n",
    "    tf_session.run(nn.optimize, {input_ph: batch_x, labels_ph: batch_y})\n",
    "\n",
    "    # check the ROC AUC score evolution every x steps\n",
    "    if training_iteration % 10 == 0:\n",
    "\n",
    "        # generate a validation batch \n",
    "        validation_x, validation_y = batchGen.random_batch(\"validation\", 500)\n",
    "\n",
    "        print(\"Predictions:\", tf_session.run(nn.predict_proba, {input_ph: validation_x,\n",
    "                                           labels_ph: validation_y}))\n",
    "\n",
    "        predicted = tf_session.run(nn.predict_proba, {input_ph: validation_x,\n",
    "                                           labels_ph: validation_y})\n",
    "\n",
    "        print(\"ROC AUC:\", roc_auc_score(validation_y.reshape(-1), predicted.reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography <a class=\"anchor\" id=\"bibliography\"></a>\n",
    "<hr/>\n",
    "\n",
    "* <span class=\"anchor\" id=\"bib-riemann\">\n",
    "    [1] Classification of covariance matrices using a Riemannian-based kernel for BCI applications <br>\n",
    "    Alexandre Barachant, Stéphane Bonnet, Marco Congedo, Christian Jutten <br>\n",
    "    https://hal.archives-ouvertes.fr/file/index/docid/820475/filename/BARACHANT_Neurocomputing_ForHal.pdf <br>\n",
    "  </span>\n",
    "<br>\n",
    "\n",
    "* <span class=\"anchor\" id=\"bib-kaggle\">\n",
    "    [2] Grasp-and-Lift EEG Detection Kaggle Competition <br>\n",
    "    https://www.kaggle.com/c/grasp-and-lift-eeg-detection <br>\n",
    "  </span>\n",
    "<br>\n",
    "\n",
    "* <span class=\"anchor\" id=\"leo-breiman\">\n",
    "    [3] Random Forests <br>\n",
    "    Leo Breiman, 2001 <br>\n",
    "    https://link.springer.com/content/pdf/10.1023%2FA%3A1010933404324.pdf <br>\n",
    "  </span>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
